{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project has been compiled via `Python3.9.10`, `conda version: 4.12.0`,  `macOS Monterey 12.1 21C52 arm64`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell can be executed to install all necessary python packages. Alternatively the requirements.txt file can be used via `\"Python3 -m pip install requirements.txt\"` command from the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install emoji typing pysentiment2 tqdm pandas numpy matplotlib sklearn nltk wordcloud varname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following two cells we import all necesarry packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "from varname.helpers import Wrapper\n",
    "\n",
    "import emoji\n",
    "from typing import List\n",
    "import pysentiment2 as ps\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sklearn.metrics as sklmx\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell makes sure that all necessary dependencies for `nltk` are being downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code can be executed to directly download the data sets from kaggle. In order to download the dataset directly from Kaggle.com the API Key `\"Kaggle.json\"` must be placed inside the working directory. This `\"Kaggle.json\"` contains my personal API key, which I is why I the disimination of this file should be kept at its neccesary. After the `\"Kaggle.json\"` has been added to the working directory (it should be in the working directory when executing the ipynb right away from the file) the following cell should be executed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the datasets have been downloaded manualy and placed in the working directory this step can and should be skipped! If it is not, the cell will through an interaction promt that can not be interacted with via a `.ipynb` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install kaggle\n",
    "! mkdir ~/.kaggle\n",
    "! cp kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "! kaggle datasets download frankcaoyun/stocktwits-2020-2022-raw\n",
    "! unzip stocktwits-2020-2022-raw.zip\n",
    "\n",
    "! kaggle datasets download harshrkh/india-financial-news-headlines-sentiments\n",
    "! unzip india-financial-news-headlines-sentiments.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Here**](https://www.kaggle.com/datasets/frankcaoyun/stocktwits-2020-2022-raw) further information about the **StockTwits dataset** can be found.  \n",
    "[**Here**](https://www.kaggle.com/datasets/harshrkh/india-financial-news-headlines-sentiments?resource=download) further information about the **News Headlines dataset** can be found.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the **News Headlines** `.csv` file and trim it down to the relevant columns (`body` & `sentiment`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_csv_file = pd.read_csv(\"News_sentiment_Jan2017_to_Apr2021.csv\")\n",
    "news_csv_file.sentiment = news_csv_file.sentiment.replace([\"NEGATIVE\", \"POSITIVE\"], [\"Bearish\",\"Bullish\"])\n",
    "unclean_NW_dataset = news_csv_file[[\"Title\", \"sentiment\",\"confidence\"]].rename(columns={\"Title\":\"body\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting all the path to the folders of the seperate Stocks that we have the **Stocktwits Data** for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "\n",
    "apple_path = path.__add__(\"/StockTwits_2020_2022_Raw/AAPL_2020_2022\")\n",
    "amazon_path = path.__add__(\"/StockTwits_2020_2022_Raw/AMZN2019-2022\")\n",
    "facebook_path = path.__add__(\"/StockTwits_2020_2022_Raw/FB_2019_2022\")\n",
    "nvidia_path = path.__add__(\"/StockTwits_2020_2022_Raw/NVDA_2013_2022\")\n",
    "tesla_path = path.__add__(\"/StockTwits_2020_2022_Raw/TSLA_2020_2022\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting all the `.csv` files from the seperate folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_csv_files = glob.glob(os.path.join(apple_path, \"*.csv\"))\n",
    "amazon_csv_files = glob.glob(os.path.join(amazon_path, \"*.csv\"))\n",
    "facebook_csv_files = glob.glob(os.path.join(facebook_path, \"*.csv\"))\n",
    "nvidia_csv_files = glob.glob(os.path.join(nvidia_path, \"*.csv\"))\n",
    "tesla_csv_files = glob.glob(os.path.join(tesla_path, \"*.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes a list of `.csv` file paths and creates a single Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allToOne(file_list:List[str]) -> pd.DataFrame():\n",
    "   \n",
    "    final_df = pd.DataFrame()\n",
    "    row_count = 0\n",
    "\n",
    "    warnings.simplefilter(action= \"ignore\", category = FutureWarning)\n",
    "\n",
    "    for f in tqdm(range(len(file_list))):\n",
    "        df = pd.read_csv(file_list[f])\n",
    "        row_count += len(df)\n",
    "        final_df = final_df.append(df)\n",
    "    \n",
    "    #print(\"full_df_length:\", len(final_df), \"|\", \"sum_of_single_df_length:\", row_count, \"|\", \"✅\" if len(final_df) == row_count else \"❌\", flush=True)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following cell we will use the previous function `allToOne` to create for every Stock a seperate Dataframe with all StockTwits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_df = allToOne(apple_csv_files)\n",
    "amazon_df = allToOne(amazon_csv_files)\n",
    "facebook_df = allToOne(facebook_csv_files)\n",
    "nvidia_df = allToOne(nvidia_csv_files)\n",
    "tesla_df = allToOne(tesla_csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(apple_df)+len(amazon_df)+len(facebook_df)+len(nvidia_df)+len(tesla_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- adjusting Labels\n",
    "- adjusting prorpotions of different types of observations (Bearish, Bullish, None)\n",
    "- cleaning text\n",
    "    - excluding filling words\n",
    "    - excluding stop words\n",
    "    - excluding links\n",
    "    - lemmatization (replacing words by lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusting the labeling of the columns (entities -> sentiment) and the cells (sentiment: {Bearish, Bullish, Non}; symbols: {AAPL, AMZN, FB, NVDA, TSLA}) and ejecting all `\"None\"` preclassified (Sentiment = None) Stocktwits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelDF(dataFrame:pd.DataFrame()) -> pd.DataFrame():\n",
    "    for i in tqdm(range(1)):\n",
    "\n",
    "        dataFrame = dataFrame[[\"body\", \"symbols\", \"entities\"]]\n",
    "        dataFrame = dataFrame.rename(columns = {\"entities\":\"sentiment\"})\n",
    "\n",
    "        dataFrame.sentiment = dataFrame.sentiment.replace(r\"^.*Bearish.*$\",\"Bearish\", regex = True)\n",
    "        dataFrame.sentiment = dataFrame.sentiment.replace(r\"^.*Bullish.*$\",\"Bullish\", regex = True)\n",
    "        dataFrame.sentiment = dataFrame.sentiment.replace(r\"^.*None.*$\",\"None\", regex = True)\n",
    "\n",
    "        dataFrame = dataFrame[dataFrame.sentiment != \"None\"]\n",
    "\n",
    "        dataFrame.symbols = dataFrame.symbols.replace(r\"^.*AAPL.*$\", \"AAPL\", regex = True)\n",
    "        dataFrame.symbols = dataFrame.symbols.replace(r\"^.*AMZN.*$\", \"AMZN\", regex = True)\n",
    "        dataFrame.symbols = dataFrame.symbols.replace(r\"^.*FB.*$\", \"FB\", regex = True)\n",
    "        dataFrame.symbols = dataFrame.symbols.replace(r\"^.*NVDA.*$\", \"NVDA\", regex = True)\n",
    "        dataFrame.symbols = dataFrame.symbols.replace(r\"^.*TSLA.*$\", \"TSLA\", regex = True)\n",
    "\n",
    "        dataFrame = dataFrame.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_apple_df = labelDF(apple_df)\n",
    "label_amazon_df = labelDF(amazon_df)\n",
    "label_facebook_df = labelDF(facebook_df)\n",
    "label_nvidia_df = labelDF(nvidia_df)\n",
    "label_tesla_df = labelDF(tesla_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function makes sure that there are an eqaual amount of StockTwits from each preclassified sentiment category (`Bullish`, `Bearish`) in the respectived datasets. To do so we randomly eject the rows of the type (`Bullis` or `Bearish`) that has more observations, until both types have the same amount of observations. We eject all StockTwits that have been classified as `None` as those can not always be definetly be determined to be nutral. Some users might forget to give their StockTwit a positv or negativ rating. Hence, `None` doesn't neccesarily state a neutral perspective of a user on a particular stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportions(dataFrame:pd.DataFrame()) -> pd.DataFrame():\n",
    "    for i in tqdm(range(1)):\n",
    "        #print(\"init length: \", len(dataFrame))\n",
    "        Bearish_dataFrame = dataFrame.loc[dataFrame[\"sentiment\"] == \"Bearish\"].sample(frac = 1, random_state=1).reset_index(drop=True)\n",
    "        Bullish_dataFrame = dataFrame.loc[dataFrame[\"sentiment\"] == \"Bullish\"].sample(frac = 1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "        min_length = min(len(Bearish_dataFrame), len(Bullish_dataFrame))\n",
    "        #print(\"bearish: \", len(Bearish_dataFrame), \"bullish: \", len(Bullish_dataFrame))\n",
    "        #print(\"min length: \", min_length, \"| 2x : \", min_length*3)\n",
    "\n",
    "        dataFrame = pd.concat([Bearish_dataFrame[:min_length],Bullish_dataFrame[:min_length]])\n",
    "\n",
    "        #print(\"end length: \", len(dataFrame))\n",
    "\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_apple_df = proportions(label_apple_df)\n",
    "cut_amazon_df = proportions(label_amazon_df)\n",
    "cut_facebook_df = proportions(label_facebook_df)\n",
    "cut_nvidia_df = proportions(label_nvidia_df)\n",
    "cut_tesla_df = proportions(label_tesla_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functinos make sure that the tweets are preprocessed and cleaned of stopwords and negations are being attached to verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df,field):\n",
    "    df[field] = df[field].str.replace(r\"http\\S+\",\" \")\n",
    "    df[field] = df[field].str.replace(r\"http\",\" \")\n",
    "    df[field] = df[field].str.replace(r\"@\",\"at\")\n",
    "    df[field] = df[field].str.replace(\"#[A-Za-z0-9_]+\", ' ')\n",
    "    df[field] = df[field].str.replace(r\"[^A-Za-z(),!?@\\'\\\"_\\n]\",\" \")\n",
    "    df[field] = df[field].str.lower()\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "STOPWORDS.update(['rt', 'mkr', 'didn', 'bc', 'n', 'm','im', 'll', 'y', 've', \n",
    "                      'u', 'ur', 'don','p', 't', 's', 'aren', 'kp', 'o', 'kat', \n",
    "                      'de', 're', 'amp', 'will'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\",text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub('[^a-zA-Z]',' ',text)\n",
    "    text = re.sub(emoji.get_emoji_regexp(),\"\",text)\n",
    "    text = re.sub(r'[^\\x00-\\x7f]','',text)\n",
    "    text = \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "    text = [lemmatizer.lemmatize(word) for word in text.split() if not word in set(STOPWORDS)]\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDF(dataFrame:pd.DataFrame, field:str) -> pd.DataFrame():\n",
    "    tqdm.pandas()\n",
    "\n",
    "    dataFrame = clean_text(dataFrame,field)\n",
    "    dataFrame.field = dataFrame[field].progress_apply(preprocess_text)\n",
    "\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we clean all the **StockTwits** datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_apple_df = cleanDF(cut_apple_df,\"body\")\n",
    "clean_amazon_df = cleanDF(cut_amazon_df,\"body\")\n",
    "clean_facebook_df = cleanDF(cut_facebook_df,\"body\")\n",
    "clean_nvidia_df = cleanDF(cut_nvidia_df,\"body\")\n",
    "clean_tesla_df = cleanDF(cut_tesla_df,\"body\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we clean the **News Headlines** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_NW_dataset = cleanDF(unclean_NW_dataset, \"body\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function concatenates the respective dataframes of each stock into one single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneDataFrame(dataFrameList:List) -> pd.DataFrame():\n",
    "    lenList = []\n",
    "    rightlenList = []\n",
    "    finalDataFrame = pd.DataFrame(lenList)\n",
    "    for i in tqdm(range(1)):\n",
    "        for i, dataFrame in enumerate(dataFrameList):\n",
    "            lenList.append(len(dataFrame))\n",
    "\n",
    "    print(lenList)   \n",
    "    minimum_length = min(lenList)\n",
    "    print(\"minimum length: \", minimum_length, \"5*: \", minimum_length*5)\n",
    "    for i, dataFrame in enumerate(dataFrameList):\n",
    "        Bearish_dataFrame = dataFrame.loc[dataFrame[\"sentiment\"] == \"Bearish\"].sample(frac = 1, random_state=1).reset_index(drop=True)\n",
    "        Bullish_dataFrame = dataFrame.loc[dataFrame[\"sentiment\"] == \"Bullish\"].sample(frac = 1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "        if minimum_length % 2 == 0:\n",
    "            Bearish_dataFrame = Bearish_dataFrame[:int(minimum_length/2)]\n",
    "            Bullish_dataFrame = Bullish_dataFrame[:int(minimum_length/2)]\n",
    "        elif minimum_length % 2 != 0:\n",
    "            Bearish_dataFrame = Bearish_dataFrame[:int(floor(minimum_length/2))]\n",
    "            Bullish_dataFrame = Bullish_dataFrame[:int(floor(minimum_length/2))]\n",
    "\n",
    "\n",
    "        dataFrame = pd.concat([Bearish_dataFrame,Bullish_dataFrame])\n",
    "        rightlenList.append(dataFrame)\n",
    "\n",
    "    for i, dataFrame in enumerate(rightlenList):\n",
    "        finalDataFrame = pd.concat([finalDataFrame,dataFrame])      \n",
    "\n",
    "    print(len(finalDataFrame))  \n",
    "\n",
    "    return finalDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_ST_dataset = oneDataFrame([clean_apple_df,clean_amazon_df, clean_facebook_df, clean_nvidia_df, clean_tesla_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we bring both datasets (**StockTwits** & **News Headlines**) to the same length by randomly ejecting observations of the **StockTwits** dataset, to make the performances of the different sentiment extraction approaches comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ST_dataset = cleaned_ST_dataset.sample(frac=1, random_state=1).reset_index(drop=True)[:len(clean_NW_dataset)]\n",
    "print(\"ST: \",len(clean_ST_dataset), \"NW: \", len(clean_NW_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two cells `can be executed` to store the **cleaned datasets** localy as well as reloading them into the notebook. This is, so that the previous lengthy process of importing and cleaning the data sets doesn't have to be redone after the kernel is being reset. This can be especially usefull when handling errors in the subsequent model training and sentiment classification process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ST_dataset.to_csv('clean_ST_dataset.csv')\n",
    "clean_NW_dataset.to_csv('clean_NW_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ST_dataset = pd.read_csv(\"clean_ST_dataset.csv\", delimiter=\",\")\n",
    "clean_NW_dataset = pd.read_csv(\"clean_NW_dataset.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_text = \" \".join(text for text in clean_ST_dataset.body)\n",
    "NW_text = \" \".join(text for text in clean_NW_dataset.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_word_cloud = WordCloud(collocations = False, background_color = 'white').generate(ST_text)\n",
    "NW_word_cloud = WordCloud(collocations = False, background_color = 'white').generate(NW_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ST_word_cloud, interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(NW_word_cloud, interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we apply different sentiment classification approaches to the two datasets and then compare their respective performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Back-of-Words) Harvard IV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that derives the Sentiment scores based of the Harvard-IV dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that derives the Sentiment scores based of the dictionary \n",
    "\n",
    "def applyHIV4two(dataFrame_init:pd.DataFrame(),none:bool) -> pd.DataFrame():\n",
    "    tqdm.pandas()\n",
    "    hav4 = ps.HIV4()\n",
    "\n",
    "    dataFrame = dataFrame_init\n",
    "\n",
    "    dataFrame[\"HAV4_polarity\"] = np.nan\n",
    "    dataFrame[\"pred_sentiment\"] = np.nan\n",
    "\n",
    "    itokenized = dataFrame.body.progress_apply(hav4.tokenize)\n",
    "    dataFrame[\"HAV4_polarity\"] = itokenized.progress_apply(hav4.get_score)\n",
    "    dataFrame[\"HAV4_polarity\"] = np.array([[r.get(\"Polarity\")] for r in dataFrame.HAV4_polarity])\n",
    "    \n",
    "    #warnings.simplefilter(action= \"ignore\", category = FutureWarning)\n",
    "    \n",
    "    if none == True:\n",
    "        dataFrame.loc[dataFrame.HAV4_polarity < 0, 'pred_sentiment'] = \"Bearish\"\n",
    "        dataFrame.loc[dataFrame.HAV4_polarity > 0, 'pred_sentiment'] = \"Bullish\"\n",
    "        dataFrame.loc[dataFrame.HAV4_polarity == 0, 'pred_sentiment'] = \"None\"\n",
    "    elif none == False:\n",
    "        dataFrame.loc[dataFrame.HAV4_polarity < 0, 'pred_sentiment'] = \"Bearish\"\n",
    "        dataFrame.loc[dataFrame.HAV4_polarity >=  0, 'pred_sentiment'] = \"Bullish\"\n",
    "\n",
    "\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDictMatrix(dictList:list,none:bool, h_l:str):\n",
    "    \n",
    "    \n",
    "    for i,model in enumerate(dictList):\n",
    "\n",
    "        label_list = [\"Bearish\",\"Bullish\",\"None\"] if none == True else [\"Bearish\",\"Bullish\"]\n",
    "        #print(\"{} Test Accuracy: {}\".format(dataFrame_list[i],model.score(X_test,y_test)))\n",
    "        cm = sklmx.confusion_matrix(model.sentiment,model.pred_sentiment, labels = label_list)\n",
    "        disp = sklmx.ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=label_list)\n",
    "        \n",
    "        disp.plot()\n",
    "        if  i == 0:\n",
    "            plt.title(f\"StockTwits: {h_l}\")\n",
    "            ST_logMc_accuracy = sklmx.accuracy_score(model.sentiment,model.pred_sentiment)\n",
    "        elif i == 1:\n",
    "            plt.title(f\"News Headlines: {h_l}\")\n",
    "            NW_logMc_accuracy = sklmx.accuracy_score(model.sentiment,model.pred_sentiment)\n",
    "\n",
    "        print(\"Accuracy: \", sklmx.accuracy_score(model.sentiment,model.pred_sentiment))\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_none_hav4_dataset = applyHIV4two(clean_ST_dataset, none = True)\n",
    "NW_none_hav4_dataset = applyHIV4two(clean_NW_dataset, none = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createDictMatrix(dictList=[ST_none_hav4_dataset,NW_none_hav4_dataset],none = True,h_l =\"HAV-IV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_hav4_dataset = applyHIV4two(clean_ST_dataset, none = False)\n",
    "NW_hav4_dataset = applyHIV4two(clean_NW_dataset, none = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createDictMatrix([ST_hav4_dataset,NW_hav4_dataset],none = False, h_l=\"HAV-IV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Back-of-Words) Loughran and McDonald"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that derives the Sentiment scores based of the dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyLogMctwo(dataFrame:pd.DataFrame(),none:bool) -> pd.DataFrame():\n",
    "    tqdm.pandas()\n",
    "    logMc = ps.LM()\n",
    "\n",
    "    dataFrame[\"LogMc_polarity\"] = np.nan\n",
    "    dataFrame[\"pred_sentiment\"] = np.nan\n",
    "\n",
    "    itokenized = dataFrame.body.progress_apply(logMc.tokenize)\n",
    "    dataFrame[\"LogMc_polarity\"] = itokenized.progress_apply(logMc.get_score)\n",
    "    dataFrame[\"LogMc_polarity\"] = np.array([[r.get(\"Polarity\")] for r in dataFrame.LogMc_polarity])\n",
    "    \n",
    "    warnings.simplefilter(action= \"ignore\", category = FutureWarning)\n",
    "\n",
    "    if none == True:\n",
    "        dataFrame.loc[dataFrame.LogMc_polarity < 0, 'pred_sentiment'] = \"Bearish\"\n",
    "        dataFrame.loc[dataFrame.LogMc_polarity > 0, 'pred_sentiment'] = \"Bullish\"\n",
    "        dataFrame.loc[dataFrame.LogMc_polarity == 0, 'pred_sentiment'] = \"None\"\n",
    "    else:\n",
    "        dataFrame.loc[dataFrame.LogMc_polarity < 0, 'pred_sentiment'] = \"Bearish\"\n",
    "        dataFrame.loc[dataFrame.LogMc_polarity >= 0, 'pred_sentiment'] = \"Bullish\"\n",
    "\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we assign the polarity scores to the dataframe and derive the sentiment labels based of the polarity scores. In the following cell we include a possible classification of `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_none_logMc_dataset = applyLogMctwo(clean_ST_dataset, True)\n",
    "NW_none_logMc_dataset = applyLogMctwo(clean_NW_dataset, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we assign the polarity scores to the dataframe and derive the sentiment labels based of the polarity scores. In the following cell we **don't** include a possible classification of `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createDictMatrix([ST_none_logMc_dataset,NW_none_logMc_dataset],True, \"Lo&Mc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_logMc_dataset = applyLogMctwo(clean_ST_dataset, False)\n",
    "NW_logMc_dataset = applyLogMctwo(clean_NW_dataset, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createDictMatrix([ST_logMc_dataset,NW_logMc_dataset], False, \"Lo&Mc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the initially cleaned and formated dataframe to create a `training` as well as `test` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_x_train, ST_x_test, ST_y_train, ST_y_test = train_test_split(clean_ST_dataset.body,clean_ST_dataset.sentiment, test_size=0.33, random_state=42)\n",
    "NW_x_train, NW_x_test, NW_y_train, NW_y_test = train_test_split(clean_NW_dataset.body, clean_NW_dataset.sentiment, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Machine Learning) Baisian Classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NB_text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('NB_clf', MultinomialNB())])\n",
    "NB_tuned_parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'NB_clf__alpha': [1, 1e-1, 1e-2]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Stocktwits Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB_training(x_train, x_test, y_train, y_test):\n",
    "\n",
    "    score = \"f1_macro\"\n",
    "    print(\"# Training model for %s\" % score)\n",
    "    print()\n",
    "    \n",
    "    np.errstate(divide='ignore')\n",
    "    NB_clf = GridSearchCV(NB_text_clf, NB_tuned_parameters, cv=2, scoring=score)\n",
    "    NB_clf.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Best set of parameters were found on the following set:\")\n",
    "    print()\n",
    "    print(NB_clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    for mean, std, params in zip(NB_clf.cv_results_['mean_test_score'], \n",
    "                                NB_clf.cv_results_['std_test_score'], \n",
    "                                NB_clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "\n",
    "    print()\n",
    "    print(classification_report(y_test, NB_clf.predict(x_test), digits=4))\n",
    "    print()\n",
    "\n",
    "    return NB_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_NB_model = NB_training(ST_x_train,ST_x_test,ST_y_train,ST_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NW_NB_model = NB_training(NW_x_train,NW_x_test,NW_y_train,NW_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createConfusionMatrix(model,x_test,y_test, name:str):\n",
    "    w_xt = Wrapper(x_test)\n",
    "    w_yt = Wrapper(y_test)\n",
    "\n",
    "    predict = model.predict(x_test)\n",
    "    cm = sklmx.confusion_matrix(y_test, predict, labels=model.classes_)\n",
    "    disp = sklmx.ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                                display_labels=model.classes_)\n",
    "    disp.plot()\n",
    "    plt.title(name)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Model Accuracy:\", model.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createConfusionMatrix(ST_NB_model,ST_x_test, ST_y_test, \"StockTwits - Naive Bayes\")\n",
    "createConfusionMatrix(NW_NB_model,NW_x_test, NW_y_test, \"News Headlines - Naive Bayes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Machine Learning) Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('SVM_clf', SVC( cache_size=2000, max_iter= 5000))])\n",
    "SVM_tuned_parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    #'clf__alpha': [1, 1e-1, 1e-2],\n",
    "    #'SVM_clf__verbose': [2],\n",
    "    #'SVM_clf__cache_size': [2000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_training(x_train, x_test, y_train, y_test):\n",
    "    score = 'f1_macro'\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "    np.errstate(divide='ignore')\n",
    "    SVM_clf = GridSearchCV(SVM_text_clf, SVM_tuned_parameters, cv=2, scoring=score)\n",
    "    SVM_clf.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(SVM_clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    for mean, std, params in zip(SVM_clf.cv_results_['mean_test_score'], \n",
    "                                SVM_clf.cv_results_['std_test_score'], \n",
    "                                SVM_clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "\n",
    "    print()\n",
    "    print(classification_report(y_test, SVM_clf.predict(x_test), digits=4))\n",
    "    print()\n",
    "\n",
    "    return SVM_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_SVM_model = SVM_training(ST_x_train,ST_x_test,ST_y_train,ST_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NW_SVM_model = SVM_training(NW_x_train,NW_x_test,NW_y_train,NW_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createConfusionMatrix(ST_SVM_model, ST_x_test, ST_y_test, \"StockTwits - Support Vector Machine\")\n",
    "createConfusionMatrix(NW_SVM_model, NW_x_test, NW_y_test, \"News Headlines - Support Vector Machine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Machine Learning) Maximum Entropy / Logistic Regression     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('LR_clf', LogisticRegression(random_state=0))])\n",
    "\n",
    "LR_tuned_parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    #'clf__alpha': [1, 1e-1, 1e-2],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_training(x_train, x_test, y_train, y_test):\n",
    "    score = 'f1_macro'\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "    np.errstate(divide='ignore')\n",
    "    LR_clf = GridSearchCV(LR_text_clf, LR_tuned_parameters, cv=2, scoring=score)\n",
    "    LR_clf.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(LR_clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    for mean, std, params in zip(LR_clf.cv_results_['mean_test_score'], \n",
    "                                LR_clf.cv_results_['std_test_score'], \n",
    "                                LR_clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "\n",
    "\n",
    "    print()\n",
    "    print(classification_report(y_test, LR_clf.predict(x_test), digits=4))\n",
    "    print()\n",
    "    return LR_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_LR_model = LR_training(ST_x_train,ST_x_test,ST_y_train,ST_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NW_LR_model = LR_training(NW_x_train,NW_x_test,NW_y_train,NW_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createConfusionMatrix(ST_LR_model, ST_x_test, ST_y_test, \"StockTwits - Logistic Regression\")\n",
    "createConfusionMatrix(NW_LR_model, NW_x_test, NW_y_test, \"News Headlines - Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Machine Learning) Multilayer Perceptron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MP_text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('MP_clf', Perceptron(tol=1e-3, random_state=0, n_jobs = 10))])\n",
    "\n",
    "MP_tuned_parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'MP_clf__alpha': [1, 1e-1, 1e-2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MP_training(x_train, x_test, y_train, y_test):\n",
    "    score = 'f1_macro'\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "    np.errstate(divide='ignore')\n",
    "    MP_clf = GridSearchCV(MP_text_clf, MP_tuned_parameters, cv=2, scoring=score)\n",
    "    MP_clf.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(MP_clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    for mean, std, params in zip(MP_clf.cv_results_['mean_test_score'], \n",
    "                                MP_clf.cv_results_['std_test_score'], \n",
    "                                MP_clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "\n",
    "    print()\n",
    "    print(classification_report(y_test, MP_clf.predict(x_test), digits=4))\n",
    "    print()\n",
    "\n",
    "    return MP_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_MP_model = MP_training(ST_x_train,ST_x_test,ST_y_train,ST_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NW_MP_model = MP_training(NW_x_train,NW_x_test,NW_y_train,NW_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createConfusionMatrix(ST_MP_model, ST_x_test, ST_y_test, \"StockTwits - Multilayer Perceptron\")\n",
    "createConfusionMatrix(NW_MP_model, NW_x_test, NW_y_test, \"News Headlines - Multilayer Perceptron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Machine Learning) Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                        hidden_layer_sizes=(5, 2)))])\n",
    "\n",
    "NN_tuned_parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': [1, 1e-1, 1e-2],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_training(x_train, x_test, y_train, y_test):\n",
    "    score = 'f1_macro'\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "    np.errstate(divide='ignore')\n",
    "    NN_clf = GridSearchCV(NN_text_clf, NN_tuned_parameters, cv=2, scoring=score)\n",
    "    NN_clf.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(NN_clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    for mean, std, params in zip(NN_clf.cv_results_['mean_test_score'], \n",
    "                                NN_clf.cv_results_['std_test_score'], \n",
    "                                NN_clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "\n",
    "\n",
    "\n",
    "    print()\n",
    "    print(classification_report(y_test, NN_clf.predict(x_test), digits=4))\n",
    "    print()\n",
    "    return NN_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_NN_model = NN_training(ST_x_train,ST_x_test,ST_y_train,ST_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NW_NN_model = NN_training(NW_x_train,NW_x_test,NW_y_train,NW_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createConfusionMatrix(ST_NN_model, ST_x_test, ST_y_test, \"StockTwits - Neural Network\")\n",
    "createConfusionMatrix(NW_NN_model, NW_x_test, NW_y_test, \"News Headlines - Neural Network\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f426dac07f1780abb603dd37a8d60962c43ae6f042cf166f353be7481077b60"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
